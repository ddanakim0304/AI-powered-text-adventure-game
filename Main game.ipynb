{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://9d492b137915d5a655.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9d492b137915d5a655.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/blocks.py\", line 1518, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/utils.py\", line 793, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gradio/chat_interface.py\", line 623, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/_2/1tb09tx55zx8v2lc5wd2_8400000gn/T/ipykernel_42658/1624781488.py\", line 141, in main_loop\n",
      "    game_state = get_game_state(inventory={\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/_2/1tb09tx55zx8v2lc5wd2_8400000gn/T/ipykernel_42658/1624781488.py\", line 12, in get_game_state\n",
      "    world = load_world('../shared_data/Kyropeia.json')\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/_2/1tb09tx55zx8v2lc5wd2_8400000gn/T/ipykernel_42658/1624781488.py\", line 8, in load_world\n",
      "    with open(filename, 'r') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 324, in _modified_open\n",
      "    return io_open(file, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '../shared_data/Kyropeia.json'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gradio as gr\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def load_world(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_game_state(inventory={}):\n",
    "    world = load_world('../shared_data/Kyropeia.json')\n",
    "    kingdom = world['kingdoms']['Eldrida']\n",
    "    town = kingdom['towns'][\"Luminaria\"]\n",
    "    character = town['npcs']['Elwyn Stormbringer']\n",
    "    start = world['start']\n",
    "\n",
    "    game_state = {\n",
    "        \"world\": world['description'],\n",
    "        \"kingdom\": kingdom['description'],\n",
    "        \"town\": town['description'],\n",
    "        \"character\": character['description'],\n",
    "        \"start\": start,\n",
    "        \"inventory\": inventory\n",
    "    }\n",
    "    return game_state\n",
    "\n",
    "class LocalLLM:\n",
    "    def __init__(self, model_path=\"meta-llama/Llama-3-8b-instruct-hf\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, \n",
    "            torch_dtype=torch.float16, \n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "\n",
    "    def generate(self, messages, max_new_tokens=200):\n",
    "        # Llama 3.1 prompt format\n",
    "        prompt = \"<|begin_of_text|>\"\n",
    "        \n",
    "        # Add system message\n",
    "        prompt += \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "        prompt += \"You are an AI Game master creating an interactive story.\\n<|eot_id|>\"\n",
    "        \n",
    "        # Add conversation history and current message\n",
    "        for msg in messages:\n",
    "            role = msg['role']\n",
    "            content = msg['content']\n",
    "            prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n{content}\\n<|eot_id|>\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        # Generate response\n",
    "        outputs = self.model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=max_new_tokens, \n",
    "            do_sample=True, \n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Decode response and remove special tokens\n",
    "        response = self.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "\n",
    "def run_action(message, history, game_state):\n",
    "    if message == 'start game':\n",
    "        return game_state['start']\n",
    "    \n",
    "    world_info = f\"\"\"\n",
    "World: {game_state['world']}\n",
    "Kingdom: {game_state['kingdom']}\n",
    "Town: {game_state['town']}\n",
    "Your Character: {game_state['character']}\n",
    "Current Inventory: {json.dumps(game_state['inventory'])}\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": world_info + \"\\n\" + message}\n",
    "    ]\n",
    "\n",
    "    # Use local LLM\n",
    "    llm = LocalLLM()\n",
    "    result = llm.generate(messages)\n",
    "    return result\n",
    "\n",
    "def detect_inventory_changes(game_state, output):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Detect inventory changes from the story. Return JSON with item updates.\"},\n",
    "        {\"role\": \"user\", \"content\": f'Current Inventory: {str(game_state[\"inventory\"])}\\nRecent Story: {output}'}\n",
    "    ]\n",
    "\n",
    "    # Use local LLM\n",
    "    llm = LocalLLM()\n",
    "    response = llm.generate(messages)\n",
    "    \n",
    "    # Parse response (add error handling)\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "        return result.get('itemUpdates', [])\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def update_inventory(inventory, item_updates):\n",
    "    update_msg = ''\n",
    "    \n",
    "    for update in item_updates:\n",
    "        name = update['name']\n",
    "        change_amount = update['change_amount']\n",
    "        \n",
    "        if change_amount > 0:\n",
    "            if name not in inventory:\n",
    "                inventory[name] = change_amount\n",
    "            else:\n",
    "                inventory[name] += change_amount\n",
    "            update_msg += f'\\nInventory: {name} +{change_amount}'\n",
    "        elif name in inventory and change_amount < 0:\n",
    "            inventory[name] += change_amount\n",
    "            update_msg += f'\\nInventory: {name} {change_amount}'\n",
    "            \n",
    "        if name in inventory and inventory[name] < 0:\n",
    "            del inventory[name]\n",
    "            \n",
    "    return update_msg\n",
    "\n",
    "def start_game(main_loop, share=False):\n",
    "    demo = gr.ChatInterface(\n",
    "        main_loop,\n",
    "        chatbot=gr.Chatbot(height=250, placeholder=\"Type 'start game' to begin\"),\n",
    "        textbox=gr.Textbox(placeholder=\"What do you do next?\", container=False, scale=7),\n",
    "        title=\"Local Llama AI RPG\",\n",
    "        theme=\"soft\",\n",
    "        examples=[\"Look around\", \"Continue the story\"],\n",
    "        cache_examples=False,\n",
    "        retry_btn=\"Retry\",\n",
    "        undo_btn=\"Undo\",\n",
    "        clear_btn=\"Clear\",\n",
    "    )\n",
    "    demo.launch(share=share, server_name=\"0.0.0.0\")\n",
    "\n",
    "def main_loop(message, history):\n",
    "    game_state = get_game_state(inventory={\n",
    "        \"cloth pants\": 1,\n",
    "        \"cloth shirt\": 1,\n",
    "        \"goggles\": 1,\n",
    "        \"leather bound journal\": 1,\n",
    "        \"gold\": 5\n",
    "    })\n",
    "\n",
    "    output = run_action(message, history, game_state)\n",
    "\n",
    "    item_updates = detect_inventory_changes(game_state, output)\n",
    "    update_msg = update_inventory(\n",
    "        game_state['inventory'], \n",
    "        item_updates\n",
    "    )\n",
    "    output += update_msg\n",
    "\n",
    "    return output\n",
    "\n",
    "# Uncomment to launch\n",
    "start_game(main_loop, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
